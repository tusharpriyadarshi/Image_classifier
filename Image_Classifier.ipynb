{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjjfnCQOQP64"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, Sequential, callbacks, models\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "from sklearn.exceptions import FitFailedWarning\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "warnings.simplefilter('always', FitFailedWarning)\n",
        "\n",
        "seed_value = 42\n",
        "np.random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "\n",
        "\n",
        "def train_test_split(data, train_split=.7, val_split=.26, test_split=.06, shuffle=True, shuffle_size=10000):\n",
        "    data_size = len(data)\n",
        "    if shuffle:\n",
        "        data = data.shuffle(shuffle_size, seed=17)\n",
        "\n",
        "    train_size = int(train_split * data_size)\n",
        "    val_size = int(val_split * data_size)\n",
        "\n",
        "    train_data = data.take(train_size)\n",
        "    val_data = data.skip(train_size).take(val_size)\n",
        "    test_data = data.skip(train_size).skip(val_size)\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "def predicts(model, img):\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = classes[np.argmax(predictions[0])]\n",
        "    confidence = round(100 * np.max(predictions[0]), 2)\n",
        "    return predicted_class, confidence\n",
        "\n",
        "\n",
        "def create_model(optimizer='adam', kernel_size=(3, 3), pool_size=(2, 2), activation='relu'):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, kernel_size=kernel_size, activation=activation, input_shape=input_shape),\n",
        "        layers.MaxPooling2D(pool_size),\n",
        "        layers.Conv2D(64, kernel_size=kernel_size, activation=activation),\n",
        "        layers.MaxPooling2D(pool_size),\n",
        "        layers.Conv2D(64, kernel_size=kernel_size, activation=activation),\n",
        "        layers.MaxPooling2D(pool_size),\n",
        "        layers.Conv2D(64, kernel_size=kernel_size, activation=activation),\n",
        "        layers.MaxPooling2D(pool_size),\n",
        "        layers.Conv2D(64, kernel_size=kernel_size, activation=activation),\n",
        "        layers.MaxPooling2D(pool_size),\n",
        "        layers.Conv2D(64, kernel_size=kernel_size, activation=activation),\n",
        "        layers.MaxPooling2D(pool_size),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation=activation),\n",
        "        layers.Dense(len(classes), activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def plot_model_accuracy(history_):\n",
        "    acc = history_.history['accuracy']\n",
        "    val_acc = history_.history['val_accuracy']\n",
        "    loss = history_.history['loss']\n",
        "    val_loss = history_.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.plot(epochs, acc, label='Training acc')\n",
        "    plt.plot(epochs, val_acc, label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, loss, label='Training loss')\n",
        "    plt.plot(epochs, val_loss, label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "data_dir = \"../input/sport-celebrity-image-classification/Sports-celebrity images\"\n",
        "img_size = 256\n",
        "batch_size = 32\n",
        "df = tf.keras.preprocessing.image_dataset_from_directory(directory=data_dir,\n",
        "                                                         shuffle=True,\n",
        "                                                         seed=seed_value,\n",
        "                                                         image_size=(img_size, img_size),\n",
        "                                                         batch_size=batch_size)\n",
        "classes = df.class_names\n",
        "\n",
        "all_files = []\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "    for file in files:\n",
        "        all_files.append(os.path.join(root, file))\n",
        "\n",
        "class_counts = {}\n",
        "for file_path in all_files:\n",
        "    class_name = os.path.basename(os.path.dirname(file_path))\n",
        "    if class_name not in class_counts:\n",
        "        class_counts[class_name] = 1\n",
        "    else:\n",
        "        class_counts[class_name] += 1\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
        "\n",
        "data_count = len(df)\n",
        "print(f\"Numbers for data: {data_count}\")\n",
        "\n",
        "for image_batch, label_batch in df.take(1):\n",
        "    print(image_batch.shape)\n",
        "    print(label_batch.numpy())\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "for image_batch, label_batch in df.take(1):\n",
        "    for i in range(12):\n",
        "        ax = plt.subplot(4, 3, i + 1)\n",
        "        plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(classes[label_batch[i]])\n",
        "        plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "train_data, val_data, test_data = train_test_split(df)\n",
        "\n",
        "print(\"Len of train:{}\".format(len(train_data)))\n",
        "print(\"Len of val:{}\".format(len(val_data)))\n",
        "print(\"Len of test:{}\".format(len(test_data)))\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=50,\n",
        "                             width_shift_range=0.2,\n",
        "                             height_shift_range=0.2,\n",
        "                             zoom_range=0.3,\n",
        "                             horizontal_flip=True,\n",
        "                             brightness_range=[.2, .5])\n",
        "\n",
        "for X_batch, y_batch in train_data:\n",
        "    for i in range(3):\n",
        "        augmented_images, augmented_labels = next(datagen.flow(X_batch, y_batch, batch_size=3))\n",
        "        X_batch = np.concatenate((X_batch, augmented_images), axis=0)\n",
        "        y_batch = np.concatenate((y_batch, augmented_labels), axis=0)\n",
        "\n",
        "train_data = train_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_data = val_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_data = test_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "channels = 3\n",
        "input_shape = (img_size, img_size, channels)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=input_shape),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(.5),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(.5),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(len(classes), activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.build(input_shape=input_shape)\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "history = model.fit(train_data,\n",
        "                    epochs=200,\n",
        "                    verbose=1,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=val_data,\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "scores = model.evaluate(test_data)\n",
        "\n",
        "plot_model_accuracy(history)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "for images, labels in test_data.take(1):\n",
        "    for i in range(12):\n",
        "        ax = plt.subplot(3, 4, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        predicted_class, confidence = predicts(model, images[i].numpy())\n",
        "        actual_class = classes[labels[i]]\n",
        "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class},\\n Confidence: {confidence}%\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "actual_list = []\n",
        "pred_list = []\n",
        "for X_batch, y_batch in test_data:\n",
        "    y_batch = y_batch.numpy().tolist()\n",
        "    y_pred = model.predict(X_batch)\n",
        "    for i in y_batch:\n",
        "        actual_list.append(i)\n",
        "    for j in y_pred:\n",
        "        pred_list.append(np.argmax(j, axis=-1).tolist())\n",
        "\n",
        "cm = confusion_matrix(actual_list, pred_list)\n",
        "sns.heatmap(cm, annot=True, cmap='seismic', fmt='g')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.show()\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "for X_batch, y_batch in train_data:\n",
        "    X_train.append(X_batch.numpy())\n",
        "    y_train.append(y_batch.numpy())\n",
        "X_train = np.concatenate(X_train)\n",
        "y_train = np.concatenate(y_train)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "optimizers = ['rmsprop', 'adam']\n",
        "kernel_size = [(3, 3), (4, 4)]\n",
        "pool_size = [(2, 2), (3, 3)]\n",
        "activation = ['relu', 'sigmoid', 'tanh']\n",
        "epochs = [60]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers,\n",
        "                  kernel_size=kernel_size,\n",
        "                  pool_size=pool_size,\n",
        "                  activation=activation,\n",
        "                  epochs=epochs)\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=1, cv=3).fit(X_train, y_train)\n",
        "\n",
        "print(\"The best score: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
        "\n",
        "channels = 3\n",
        "input_shape = (img_size, img_size, channels)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation=\"tanh\", input_shape=input_shape),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"tanh\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(.5),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"tanh\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"tanh\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(.5),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"tanh\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"tanh\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation=\"tanh\"),\n",
        "    layers.Dense(len(classes), activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.build(input_shape=input_shape)\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
        "\n",
        "history = model.fit(train_data,\n",
        "                    epochs=300,\n",
        "                    verbose=1,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=val_data,\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "scores = model.evaluate(test_data)\n",
        "\n",
        "plot_model_accuracy(history)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "for images, labels in test_data.take(1):\n",
        "    for i in range(12):\n",
        "        ax = plt.subplot(3, 4, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        predicted_class, confidence = predicts(model, images[i].numpy())\n",
        "        actual_class = classes[labels[i]]\n",
        "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class},\\n Confidence: {confidence}%\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "actual_list = []\n",
        "pred_list = []\n",
        "for X_batch, y_batch in test_data:\n",
        "    y_batch = y_batch.numpy().tolist()\n",
        "    y_pred = model.predict(X_batch)\n",
        "    for i in y_batch:\n",
        "        actual_list.append(i)\n",
        "    for j in y_pred:\n",
        "        pred_list.append(np.argmax(j, axis=-1).tolist())\n",
        "\n",
        "cm = confusion_matrix(actual_list, pred_list)\n",
        "sns.heatmap(cm, annot=True, cmap='seismic', fmt='g')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.show()\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "for X_batch, y_batch in train_data:\n",
        "    X_train.append(X_batch.numpy())\n",
        "    y_train.append(y_batch.numpy())\n",
        "X_train = np.concatenate(X_train)\n",
        "y_train = np.concatenate(y_train)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "optimizers = ['rmsprop', 'adam']\n",
        "kernel_size = [(3, 3), (4, 4)]\n",
        "pool_size = [(2, 2), (3, 3)]\n",
        "activation = ['relu', 'sigmoid', 'tanh']\n",
        "epochs = [60]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers,\n",
        "                  kernel_size=kernel_size,\n",
        "                  pool_size=pool_size,\n",
        "                  activation=activation,\n",
        "                  epochs=epochs)\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=1, cv=3).fit(X_train, y_train)\n",
        "\n",
        "print(\"The best score: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
        "\n",
        "channels = 3\n",
        "input_shape = (img_size, img_size, channels)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=input_shape),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(len(classes), activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.build(input_shape=input_shape)\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(train_data,\n",
        "                    epochs=200,\n",
        "                    verbose=1,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=val_data)\n",
        "\n",
        "scores = model.evaluate(test_data)\n",
        "\n",
        "plot_model_accuracy(history)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "for images, labels in test_data.take(1):\n",
        "    for i in range(12):\n",
        "        ax = plt.subplot(3, 4, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        predicted_class, confidence = predicts(model, images[i].numpy())\n",
        "        actual_class = classes[labels[i]]\n",
        "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class},\\n Confidence: {confidence}%\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "actual_list = []\n",
        "pred_list = []\n",
        "for X_batch, y_batch in test_data:\n",
        "    y_batch = y_batch.numpy().tolist()\n",
        "    y_pred = model.predict(X_batch)\n",
        "    for i in y_batch:\n",
        "        actual_list.append(i)\n",
        "    for j in y_pred:\n",
        "        pred_list.append(np.argmax(j, axis=-1).tolist())\n",
        "\n",
        "cm = confusion_matrix(actual_list, pred_list)\n",
        "sns.heatmap(cm, annot=True, cmap='seismic', fmt='g')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.show()\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "for X_batch, y_batch in train_data:\n",
        "    X_train.append(X_batch.numpy())\n",
        "    y_train.append(y_batch.numpy())\n",
        "X_train = np.concatenate(X_train)\n",
        "y_train = np.concatenate(y_train)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "optimizers = ['rmsprop', 'adam']\n",
        "kernel_size = [(3, 3), (4, 4)]\n",
        "pool_size = [(2, 2), (3, 3)]\n",
        "activation = ['relu', 'sigmoid', 'tanh']\n",
        "epochs = [60]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers,\n",
        "                  kernel_size=kernel_size,\n",
        "                  pool_size=pool_size,\n",
        "                  activation=activation,\n",
        "                  epochs=epochs)\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=1, cv=3).fit(X_train, y_train)\n",
        "\n",
        "print(\"The best score: %f using %s\" % (grid.best_score_, grid.best_params_))\n"
      ]
    }
  ]
}